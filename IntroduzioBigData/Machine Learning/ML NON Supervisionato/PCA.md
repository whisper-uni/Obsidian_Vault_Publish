
![[PCA_mappa.png]]

Una delle diverse tipologie di [[ML Non Supervisionato]] Ã¨ la Principal Components Analysis (PCA). ==La PCA ha lâ€™obiettivo di derivare, dato un insieme di variabili p su un certo numero n di osservazioni, un set di features con dimensione minore==. Quindi date n osservazioni in uno spazio p  dimensioni siano interessanti tutte allo stesso modo. Questo puÃ² succedere perchÃ© ==ci sono delle variabili che dipendono dalle altre, e possono essere ottenute come combinazione di un sottoinsieme di variabili==. Dunque si cerca il ==set minimo di variabili che descrivono ugualmente bene, o nei limiti di una certa ==[[Varianza]], lâ€™intero set n di osservazioni date. La PCA vuole fare una valutazione alquanto ==geometrica: ridurre la dimensione di una data matrice nxp identificando le direzioni delle features nello spazio p dimensionale attraverso cui i dati originali (quindi le n osservazioni) siano altamente variabili(con la piÃ¹ alta [[Varianza]]==) .


![[Immagine PCA.png]]

La prima componente principale altro non Ã¨ che la combinazione lineare normalizzata delle features, quindi delle p colonne della matrice X che ha la piÃ¹ ampia varianza

Le p nuove variabili sono dette componenti principali (o principal components). 
â¢ Caratteristiche delle componenti principali: 
	â–ª Le **componenti principali sono nuove variabili**, create artificialmente, nessuna di loro coincide con alcuna delle m variabili di partenza. 
	â–ª Ogni componente principale Ã¨ una **combinazione lineare delle m variabili originali.** 
	â–ª Le **componenti principali sono tali da riassumere quanta piÃ¹ informazione possibile sulle m variabili originali. **
	â–ª Le componenti principali sono ordinate in base a quanta informazione del dataset originale racchiudono (ğ‘ƒğ¶1 Ã¨ la componente piÃ¹ informativa, ğ‘ƒğ¶2 la seconda piÃ¹ informativa ecc.) 
	â–ª Le componenti principali per costruzione sono tra loro scorrelate.**
	In pratica la PCA effettua una proiezione ortogonale dei dati su uno spazio definito da nuove dimensioni dette componenti principali. Queste sono tali per cui la varianza delle coordinate dei dati proiettati sulle nuove dimensioni Ã¨ massima per le prime dimensioni.
	![[Pasted image 20250527235031.png]]

### PerchÃ¨ utilizzare la PCA?
Visualizzazione
 Quando abbiamo dataset con tante variabili (m grande) diventa difficile rappresentare graficamente i dati per analizzarli dal punto di vista visivo. 
	 â–ª Tipicamente si visualizzano le distribuzioni delle singole variabili o al piÃ¹ gli scatterplot di coppie di variabili. 
	 â–ª Problemi: 
		 â€¢ Lo scatterplot di due variabili rappresenta solo una piccola quantitÃ  dellâ€™informazione contenuta nei dati. 
		 â€¢ Con m variabili dovremmo fare ğ‘š âˆ™ (ğ‘š âˆ’ 1)/2 scatterplot. Se m = 10 â†’ 10 x 9/2 = 45 scatterplot!
	â¢ Possiamo sfruttare la PCA per riassumere lâ€™informazione contenuta nei dati usando poche variabili, piÃ¹ semplici da rappresentare. 
		â–ª Potremmo realizzare lo scatterplot delle prime 2 componenti principali, ğ‘ƒğ¶1 e ğ‘ƒğ¶2, ovvero quelle piÃ¹ informative
Compressione Dati
	â¢ Compressione: al posto di archiviare le m variabili di partenza, archivio le p componenti principali, con un risparmio in memoria.	
	â¢ Decompressione: utilizzando le p componenti principali ricostruisco le m variabili di partenza. La ricostruzione non sarÃ  perfetta, lâ€™errore introdotto dalla compressione dipende da quanto informative erano le p componenti principali selezionate

### PCA, COME SI REALIZZA?

â¢ Dataset originale: m variabili, ğ‘‹1, ğ‘‹2, â€¦ , ğ‘‹ğ‘š, a media nulla. 
â¢ Nota: se le variabili non sono a media nulla, prima di realizzare la PCA, i dati vanno centrati â†’ ad ogni variabile va sottratta la sua media. 

![[Pasted image 20250527234308.png]]

â¢ Trasformazione lineare normalizzata delle m variabili â†’ m nuove variabili, ğ‘ƒğ¶1, ğ‘ƒğ¶2, â€¦,ğ‘ƒğ¶ğ‘š, dette componenti principali tra loro scorrelate 
	ğ‘ƒğ¶1 = ğ‘£11 ğ‘‹1 + ğ‘£21 ğ‘‹2 + â€¦ + ğ‘£ğ‘š1 ğ‘‹ğ‘š 
	ğ‘ƒğ¶2 = ğ‘£12 ğ‘‹1 + ğ‘£22 ğ‘‹2 + â€¦ + ğ‘£ğ‘š2 ğ‘‹ğ‘š 
	ğ‘ƒğ¶ğ‘š = ğ‘£1ğ‘š ğ‘‹1 + ğ‘£2ğ‘š ğ‘‹2 + â€¦ + ğ‘£ğ‘šğ‘š ğ‘‹ğ‘š 
â¢ I coefficienti ğ‘£ğ‘–ğ‘˜ si dicono loadings consentono di trasformare le variabili di partenza nelle nuove variabili, le componenti principali.
 I loadings devono essere tali che le prime componenti principali riassumano quanta piÃ¹ varianza possibile delle variabili di partenza: 
 ğ‘£ğ‘ğ‘Ÿ ğ‘ƒğ¶1 > ğ‘£ğ‘ğ‘Ÿ ğ‘ƒğ¶2 > ğ‘£ğ‘ğ‘Ÿ ğ‘ƒğ¶3 > â‹¯ > ğ‘£ğ‘ğ‘Ÿ ğ‘ƒğ¶ğ‘š 
 â–ª ğ‘ƒğ¶1  da sola deve essere in grado di spiegare quanta piÃ¹ varianza possibile dei dati di partenza. 
 â–ª ğ‘ƒğ¶2 da sola deve essere in grado di spiegare quanta piÃ¹ varianza possibile della porzione di varianza non spiegata da ğ‘ƒğ¶1. 
 â–ª â€¦ 
 â¢ Considerando solo le prime p componenti principali possiamo efficacemente ridurre la dimensionalitÃ  dei dati â†’ nuovo set di p<m variabili che riassume la maggior parte della [[Varianza]] delle m variabili di partenza. 

 Per passare dalle coordinate dei punti nello spazio originale a quelle nello spazio definito dalle componenti principali, basta applicare la trasformazione lineare definita dai loadings

![[Pasted image 20250527233816.png]]

   ğ’€  = ğ‘¿ âˆ™ ğ‘½ 
 â¢ ==La matrice V Ã¨ di fatto una matrice di rotazione che consente di passare dalle coordinate nel sistema di riferimento originale, alle coordinate nel sistema di riferimento delle componenti principali.==

 La PCA ci fornisce dunque una nuova rappresentazione dei dati nello spazio delle componenti principali, ğ‘ƒğ¶1, ğ‘ƒğ¶2, â€¦,ğ‘ƒğ¶ğ‘š, che ha 2 caratteristiche fondamentali: 
 â–ª Le componenti principali sono scorrelate 
 â–ª La varianza dei dati proiettati lungo le componenti principali decresce allâ€™aumentare delle componenti, la maggior parte della varianza complessiva Ã¨ concentrata nelle prime componenti principali ğ‘£ğ‘ğ‘Ÿ ğ‘ƒğ¶1 > ğ‘£ğ‘ğ‘Ÿ ğ‘ƒğ¶2 > ğ‘£ğ‘ğ‘Ÿ ğ‘ƒğ¶3 > â‹¯ > ğ‘£ğ‘ğ‘Ÿ ğ‘ƒğ¶ğ‘š 
 â¢ Per ridurre la dimensionalitÃ  possiamo considerare solo le prime p componenti principali

COME SI CALCOLANO I LOADINGS?

 Come possiamo calcolare i valori dei loadings, ovvero la matrice V, che mi consente di realizzare la PCA? 
 â¢ Le colonne della matrice V sono gli autovettori della matrice di covarianza di X ordinati secondo lâ€™ordine decrescente dei rispettivi autovalori.
Calcolo della [[matrice di covarianza]] di X: S
â¢ Calcoliamo autovalori e autovettori di S.
	Sia ğ‘¨ âˆˆ â„ ğ‘Ã—ğ‘ una matrice quadrata di ğ‘ righe e ğ‘ colonne. Se esistono un vettore ğ’— âˆˆ â„ğ‘ e uno scalare ğœ† (anche complesso) tali che: 
	ğ‘¨ğ’— = ğœ†ğ’— si dice che ğ’— Ã¨ autovettore di ğ‘¨ e ğœ† il suo autovalore corrispondente. 
	ProprietÃ : 
	â¢ Una matrice di dimensione N x N ha al massimo N autovalori distinti. 
	â¢ Gli autovalori sono gli zeri del polinomio caratteristico: 
	det (ğ‘¨ âˆ’ ğœ†ğ‘° )= 0
	
Gli autovettori di S ([[matrice di covarianza]]) rappresentano i loadings che definiscono le componenti principali. 
â¢ Le componenti principali sono ortogonali tra loro â†’ quindi scorrelate. 
â¢ In che ordine vengono considerati gli autovettori per definire le componenti principali? â†’ In base agli autovalori corrispondenti.

â¢ Ordiniamo i gli autovalori dal piÃ¹ grande al piÃ¹ piccolo: 
ğœ†1 > ğœ†2 > â‹¯ > ğœ†ğ‘š 
 |.        |               |
ğ’—ğŸ      ğ’—ğŸ â€¦       ğ’—ğ’ 
â¢ Lâ€™autovettore ğ’—ğŸ corrispondente allâ€™autovalore massimo, ğœ†1, rappresenta il vettore dei loadings della prima componente principale, ovvero la prima colonna di ğ‘½. 
â¢ Gli autovettori, ğ’—ğŸ, ğ’—ğŸ, â€¦ , ğ’—ğ’, in questo ordine definiscono le colonne di ğ‘½: ğ‘½=\[ğ’—ğŸ ğ’—ğŸ â€¦ ğ’—ğ’\] 
==Gli autovalori rappresentano la varianza dei dati proiettati lungo le componenti principali==


### PCA - RIASSUNTO

1. Centramento (ad ogni colonna di X si sottrae la sua media) o  [[Standardizazione]] dei dati:
2. Calcolo della [[matrice di covarianza]] di X â†’ S (m x m) 
3. Calcolo di autovettori e autovalori di S costruendo la matrice V dei loadings. 
		â–ª Gli autovettori rappresentano i coefficienti per definire le componenti principali. 
		â–ª Lâ€™ordine delle componenti principali Ã¨ stabilito dallâ€™ordine degli autovalori. 
		â–ª Lâ€™autovettore corrispondente allâ€™autovalore massimo rappresenta i coefficienti (loadings) della prima componente principale. 
4. Trasformazione dei dati passando alle coordinate nello spazio delle componenti principali: Y=XÂ·V 
5. Scelta delle prime p componenti principali che rappresentano gran parte della varianza delle variabili originali (scree plot o plot della frazione di varianza spiegata dalle prime p componenti). 
6. Le coordinate delle n osservazioni lungo le p componenti principali rappresentano il dataset trasformato e ridotto. 

### SCELTA DEL NUMERO DI COMPONENTI PRINCIPALI (SCREE PLOT)

![[Pasted image 20250528003511.png]]